# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17PnPsbZeebxv2E4Xyjg823yPb1ES8yea
"""

import pandas as pd
import numpy as np

health2=pd.read_csv('health2.csv')

health2.head()

health2.dtypes

from sklearn.model_selection import train_test_split

# Convert categorical "Yes"/"No" responses into numerical values (Yes = 1, No = 0)
health2 = health2.applymap(lambda x: 1 if x == "Yes" else (0 if x == "No" else x))

# Check for missing or inconsistent data
print("\nChecking for missing values:")
missing_data = health2.isnull().sum()
print(missing_data)

# Optionally: Handle missing values (Here we fill missing values with the column mean)
health2 = health2.fillna(health2.mean())

# Display the processed data
print("\nProcessed Data:")
print(health2.head(46))

# Save the processed data to a new CSV file (optional)
processed_file_path = '/content/health2.csv'
health2.to_csv(processed_file_path, index=False)

health2=pd.read_csv('health2.csv')

# Feature Selection: Select relevant features (predictors) for mental health prediction
features = [
    'Do you feel your work-life balance is satisfactory?',
    '  Do you often feel motivated to complete your tasks at work?  ',
    'Do you feel comfortable discussing your mental health concerns with your supervisor or colleagues?',
    '  Do you regularly engage in physical activity or exercise?  ',
    'Have you never experienced any significant changes in your relationships due to work-related stress?'
]

# Target Variable: Predict mental health related to exhaustion
target = 'Do you never find yourself frequently feeling exhausted or drained at the end of the workday?'

# Separate the predictors (X) and the target variable (y)
X = health2[features]
y = health2[target]

# Display the predictors and target to verify
print("Selected Features (Predictors):")
print(X.head())
print("\nTarget Variable (Mental Health - Exhaustion):")
print(y.head())

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display the size of the training and testing sets
print(f"\nTraining set size: {X_train.shape}")
print(f"Testing set size: {X_test.shape}")

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split

# Step 6: Split the dataset into 80% training and 20% testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 7: Display the size of the training and testing sets
print(f"Training set size (X_train): {X_train.shape}")
print(f"Training set size (y_train): {y_train.shape}")
print(f"Testing set size (X_test): {X_test.shape}")
print(f"Testing set size (y_test): {y_test.shape}")

from sklearn.ensemble import RandomForestClassifier

# Step 6: Initialize and train the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Step 7: Make predictions on the test data
y_pred = rf_model.predict(X_test)

from sklearn.metrics import accuracy_score, classification_report

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f'Accuracy: {accuracy * 100:.2f}%')
print(f'Precision: {precision * 100:.2f}%')
print(f'Recall: {recall * 100:.2f}%')
print(f'F1-Score: {f1 * 100:.2f}%')

print('\nClassification Report:')
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Generate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix using Seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

from sklearn.model_selection import cross_val_score

scores = cross_val_score(rf_model, X, y, cv=5) # Example with 5 folds

print("Cross-validation scores:", scores)
print("Average cross-validation score:", scores.mean())

"""Saving the model as pickle file and convert to zip file"""

import pickle
with open("model.pkl","wb") as f:
  pickle.dump(rf_model,f)

"""To add schema into this model"""

# Get the number of trees in the forest
n_trees = rf_model.n_estimators

   # Get the maximum depth of the trees
max_depth = rf_model.max_depth

   # Get the number of features used for splitting
n_features = rf_model.n_features_in_

   # Print the information
print(f"Number of trees: {n_trees}")
print(f"Maximum depth: {max_depth}")
print(f"Number of features: {n_features}")

from sklearn.tree import plot_tree

   # Visualize the first tree in the forest
plt.figure(figsize=(12, 8))
plot_tree(rf_model.estimators_[0], feature_names=features, filled=True)
plt.show()

feature_importances = rf_model.feature_importances_
print(feature_importances)

import pickle
with open("model.pkl","wb") as f:
  pickle.dump(rf_model,f)

!pip install sklearn2pmml==0.88.0

pip install --upgrade sklearn2pmml numpy scikit-learn

import pickle

with open("model.pkl", "rb") as f:
    rf_model = pickle.load(f)

!pip install skl2onnx

import pickle

with open("model.pkl", "rb") as f:
    rf_model = pickle.load(f)

print(type(rf_model).__name__)

!pip install skl2onnx

from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType
initial_type = [('float_input', FloatTensorType([None, 4]))]  # Replace 4 with the number of features
onx = convert_sklearn(rf_model, initial_types=initial_type)
with open("rf_model.onnx", "wb") as f:
    f.write(onx.SerializeToString())

!pip install onnx onnxruntime

import torch
import torchvision.models as models

# Example: Load a pre-trained model
model = models.resnet18(pretrained=True)
model.eval()

dummy_input = torch.randn(1, 3, 224, 224)  # Adjust input size as needed
torch.onnx.export(model, dummy_input, "model.onnx", opset_version=11)

import onnx
import onnxruntime as ort

# Load the ONNX model
onnx_model = onnx.load("model.onnx")
onnx.checker.check_model(onnx_model)

# Run a test inference
ort_session = ort.InferenceSession("model.onnx")
input_name = ort_session.get_inputs()[0].name
outputs = ort_session.run(None, {input_name: dummy_input.numpy()})
print(outputs)

"""To know current input name for conversion"""

import onnx

onnx_model = onnx.load("model.onnx")
for input in onnx_model.graph.input:
    print(input.name)

import onnx
import onnxruntime as ort

# Load the ONNX model
onnx_model = onnx.load("model.onnx")
onnx.checker.check_model(onnx_model)

# Run a test inference
ort_session = ort.InferenceSession("model.onnx")
outputs = ort_session.run(None, {"input.1": dummy_input.numpy()})
print(outputs)

import zipfile

with zipfile.ZipFile("model.zip", "w") as zipf:
    zipf.write("model.onnx")

from google.colab import files
files.download("model.zip")

import torch
import torch.onnx
import torchvision.models as models

# Load your model (example with ResNet)
model = models.resnet18(pretrained=True)
model.eval()

# Create a dummy input tensor
dummy_input = torch.randn(1, 3, 224, 224)

# Export the model to ONNX format
torch.onnx.export(model, dummy_input, "model.onnx")

import onnx

# Load the ONNX model
onnx_model = onnx.load("model.onnx")

# Check that the model is well-formed
onnx.checker.check_model(onnx_model)

import zipfile

# Create a zip file
with zipfile.ZipFile('model.zip', 'w') as zipf:
    zipf.write('model.onnx')

from google.colab import files

# Download the zip file
files.download('model.zip')

import torch
import torch.onnx
import torchvision.models as models

# Load a simpler model
model = models.alexnet(pretrained=True)
model.eval()

# Create a dummy input tensor
dummy_input = torch.randn(1, 3, 224, 224)

# Export the model to ONNX format
torch.onnx.export(model, dummy_input, "simple_model.onnx")

import onnx

# Load the ONNX model
onnx_model = onnx.load("simple_model.onnx")

# Check that the model is well-formed
onnx.checker.check_model(onnx_model)

import zipfile
from google.colab import files

# Create a zip file
with zipfile.ZipFile('simple_model.zip', 'w') as zipf:
    zipf.write('simple_model.onnx')

# Download the zip file
files.download('simple_model.zip')

!pip install ibm-watson-machine-learning

import requests

# Replace with your actual IAM token - DO NOT HARDCODE IN PRODUCTION
iam_token = "myapikey"
request_method = "GET"
method_endpoint = "/ml/v4/models"

headers = {
    "Authorization": f"Bearer {iam_token}"
}

url = f"https://us-south.ml.cloud.ibm.com{method_endpoint}"

response = requests.request(request_method, url, headers=headers)

print(response.text)

